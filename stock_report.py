import pandas as pd
import yfinance as yf
from pykrx import stock
from datetime import datetime, timedelta
import os
import requests
from bs4 import BeautifulSoup
import time

def get_sector_data():
    return {
        "ë°˜ë„ì²´": ['005930.KS', '000660.KS', '042700.KS', '058470.KQ', '000990.KS'],
        "ì´ì°¨ì „ì§€": ['373220.KS', '006400.KS', '051910.KS', '247540.KQ', '003670.KS'],
        "ìë™ì°¨": ['005380.KS', '000270.KS', '012330.KS', '011280.KS'],
        "ë°”ì´ì˜¤": ['207940.KS', '068270.KS', '000100.KS', '326030.KS'],
        "ì¸í„°ë„·/í”Œë«í¼": ['035420.KS', '035720.KS', '323410.KS', '377300.KS'],
        "ê¸ˆìœµ": ['105560.KS', '055550.KS', '086790.KS', '316140.KS', '003550.KS'],
        "ì² ê°•": ['005490.KS', '004020.KS', '016380.KS'],
        "í†µì‹ ": ['017670.KS', '030200.KS', '032640.KS'],
        "ê²Œì„": ['259960.KS', '036570.KS', '251270.KQ', '063080.KQ'],
        "ì—”í„°í…Œì¸ë¨¼íŠ¸": ['352820.KS', '035900.KQ', '041510.KQ', '122870.KQ'],
        "í™”ì¥í’ˆ": ['051900.KS', '002790.KS', '192820.KS', '161890.KS', '214320.KS']
    }

def get_naver_investor_data(ticker_code):
    """
    Naver Financeì—ì„œ íˆ¬ììë³„ ë§¤ë§¤ë™í–¥(ìˆ˜ëŸ‰)ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    code = ticker_code.split('.')[0]
    url = f"https://finance.naver.com/item/frgn.naver?code={code}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        res = requests.get(url, headers=headers)
        # Naver Finance uses EUC-KR (CP949)
        content = res.content.decode('cp949', 'ignore')
        soup = BeautifulSoup(content, 'html.parser')
        
        # 'type2' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸” ì¤‘ 'ë‚ ì§œ'ê°€ í¬í•¨ëœ í…Œì´ë¸” ì°¾ê¸°
        tables = soup.find_all('table', class_='type2')
        target_table = None
        for t in tables:
            if 'ë‚ ì§œ' in t.text:
                target_table = t
                break
        
        if not target_table:
            return None
            
        rows = target_table.find_all('tr')
        data = []
        for row in rows:
            cols = row.find_all('td')
            # ìµœì†Œ 7ê°œ ì´ìƒì˜ ì»¬ëŸ¼ì´ ìˆì–´ì•¼ ë°ì´í„° ë¡œìš°ì„
            if len(cols) < 7: continue
            
            date_str = cols[0].text.strip().replace('.', '')
            if not date_str or len(date_str) != 8: continue
            
            try:
                volume = int(cols[4].text.replace(',', ''))
                inst_net = int(cols[5].text.replace(',', ''))
                for_net = int(cols[6].text.replace(',', ''))
                # ê°œì¸ì€ (ê¸°ê´€+ì™¸êµ­ì¸)ì˜ ë°˜ëŒ€ë¡œ ì¶”ì‚°
                ind_net = -(inst_net + for_net)
                
                data.append({
                    'ë‚ ì§œ': date_str,
                    'ê±°ë˜ëŸ‰': volume,
                    'ê¸°ê´€': inst_net,
                    'ì™¸êµ­ì¸': for_net,
                    'ê°œì¸': ind_net
                })
            except:
                continue
        return pd.DataFrame(data)
    except Exception as e:
        print(f"Error scraping {ticker_code}: {e}")
        return None

def get_stats_yf_and_naver(tickers):
    data = yf.download(tickers, period="60d", interval="1d", progress=False)
    if data.empty:
        return None
        
    def get_ticker_df(t):
        if len(tickers) > 1:
            try:
                return data.xs(t, axis=1, level=1)
            except:
                return pd.DataFrame()
        return data

    # Pre-fetch Naver data
    naver_dfs = {}
    for t in tickers:
        df = get_naver_investor_data(t)
        naver_dfs[t] = df
        time.sleep(0.1)

    dates_yf = data.index.strftime("%Y%m%d").tolist()
    
    def calc_period_metrics(t_list, start_idx, end_idx, base_end_idx):
        prices = []
        volumes = []
        
        ind_v_sum, for_v_sum, ins_v_sum, total_v_sum = 0, 0, 0, 0
        
        start_date_str = dates_yf[start_idx]
        end_date_str = dates_yf[end_idx]
        
        for t in t_list:
            tdf = get_ticker_df(t)
            if tdf.empty: continue
            
            try:
                # Price change: (End Close - Base Close) / Base Close
                curr_p = tdf['Close'].iloc[end_idx]
                prev_p = tdf['Close'].iloc[base_end_idx]
                if prev_p > 0:
                    prices.append((curr_p - prev_p) / prev_p * 100)
                
                # Absolute Volume for the period
                s = start_idx
                e = end_idx + 1 if end_idx != -1 else None
                v_sum = tdf['Volume'].iloc[s:e].sum()
                if not pd.isna(v_sum):
                    volumes.append(v_sum)
                
                # Investor data from Naver
                ndf = naver_dfs.get(t)
                if ndf is not None and not ndf.empty:
                    mask = (ndf['ë‚ ì§œ'] >= start_date_str) & (ndf['ë‚ ì§œ'] <= end_date_str)
                    period_ndf = ndf.loc[mask]
                    if not period_ndf.empty:
                        ind_v_sum += period_ndf['ê°œì¸'].sum()
                        for_v_sum += period_ndf['ì™¸êµ­ì¸'].sum()
                        ins_v_sum += period_ndf['ê¸°ê´€'].sum()
                        total_v_sum += period_ndf['ê±°ë˜ëŸ‰'].sum()
            except:
                continue
        
        avg_price = sum(prices) / len(prices) if prices else 0
        sum_vol = sum(volumes) if volumes else 0
        
        return {
            "ê°€ê²©%": round(avg_price, 2),
            "ê±°ë˜ëŸ‰": int(sum_vol),
            "ê°œì¸": int(ind_v_sum),
            "ì™¸ì¸": int(for_v_sum),
            "ê¸°ê´€": int(ins_v_sum)
        }

    # Results: Today, Yesterday, and Weekly
    # Today vs Yesterday Close
    res_t = calc_period_metrics(tickers, -1, -1, -2)
    # Yesterday vs Day Before yesterday Close
    res_y = calc_period_metrics(tickers, -2, -2, -3)
    # Week (last 5 days) vs 6th day ago Close
    res_w = calc_period_metrics(tickers, -5, -1, -6)

    results = {
        "ë‹¹ì¼": res_t,
        "ì–´ì œ": res_y,
        "ì£¼ê°„": res_w
    }
    return results

import xml.etree.ElementTree as ET
import urllib.parse

def get_sector_news(sector):
    """
    ì„¹í„°ë³„ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ë˜, ë¦¬í¬íŠ¸ ì‘ì„±ì¼(ì˜¤ëŠ˜) ê¸°ì¤€ì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    ìœ ì‚¬í•œ ì œëª©ì˜ ì¤‘ë³µ ë‰´ìŠ¤ë„ ì œê±°í•©ë‹ˆë‹¤.
    """
    query = f"{sector} ì£¼ì‹ ë‰´ìŠ¤"
    encoded_query = urllib.parse.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    # ì˜¤ëŠ˜ ë‚ ì§œ (ë¬¸ìì—´ ë¹„êµìš©) - ì˜ˆ: "06 Feb 2026"
    today = datetime.now()
    today_str = today.strftime("%d %b %Y") 
    
    try:
        res = requests.get(url, headers=headers, timeout=10)
        root = ET.fromstring(res.content)
        
        raw_news = []
        for item in root.findall('.//item'):
            pub_date = item.find('pubDate').text
            # Google News RSS pubDate format: "Fri, 06 Feb 2026 07:10:00 GMT"
            # ì˜¤ëŠ˜ ë‰´ìŠ¤ì¸ì§€ í™•ì¸ (ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ)
            if today_str not in pub_date:
                continue
                
            title = item.find('title').text
            link = item.find('link').text
            if ' - ' in title:
                title = title.rsplit(' - ', 1)[0]
            raw_news.append({'title': title, 'link': link})
            if len(raw_news) >= 15: # ì¶©ë¶„íˆ í™•ë³´ í›„ í•„í„°ë§
                break
            
        # ì¤‘ë³µ/ìœ ì‚¬ ì œëª© í•„í„°ë§ ë¡œì§
        filtered_news = []
        seen_titles = []
        
        for news in raw_news:
            is_duplicate = False
            current_words = set(news['title'].split())
            
            for prev_title in seen_titles:
                prev_words = set(prev_title.split())
                intersection = current_words.intersection(prev_words)
                if len(intersection) / max(1, min(len(current_words), len(prev_words))) > 0.5:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_news.append(f"- {news['title']} ([ë§í¬]({news['link']}))")
                seen_titles.append(news['title'])
                if len(filtered_news) >= 3:
                    break
                    
        return filtered_news if filtered_news else ["- ì˜¤ëŠ˜ ë“±ë¡ëœ ì£¼ìš” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."]
    except:
        return ["- ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

def generate_summary(df, sector_news):
    """
    ë¦¬í¬íŠ¸ ë°ì´í„°ì™€ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    summary = "## ğŸ“ ì‹œì¥ ë¶„ì„ ìš”ì•½\n\n"
    
    # 1. ì£¼ê°„ ìµœê³  ìƒìŠ¹ ì„¹í„°
    weekly_top = df.loc[df['ì£¼ê°„_ê°€ê²©%'].idxmax()]
    summary += f"### ğŸš€ ì£¼ê°„ ë² ìŠ¤íŠ¸ ì„¹í„°: **{weekly_top['ì„¹í„°']}**\n"
    summary += f"- ì§€ë‚œ ì¼ì£¼ì¼ê°„ í‰ê·  **{weekly_top['ì£¼ê°„_ê°€ê²©%']}%** ìƒìŠ¹í•˜ë©° ê°€ì¥ ê°•í•œ íë¦„ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n"
    
    # 2. ë‹¹ì¼ íŠ¹ì´ ì‚¬í•­
    today_up = df[df['ë‹¹ì¼_ê°€ê²©%'] > 0]
    if not today_up.empty:
        top_today = today_up.loc[today_up['ë‹¹ì¼_ê°€ê²©%'].idxmax()]
        summary += f"### ğŸ“ˆ ë‹¹ì¼ ê°•ì„¸ ì„¹í„°: **{top_today['ì„¹í„°']}**\n"
        summary += f"- ì˜¤ëŠ˜ ì‹œì¥ì—ì„œ **{top_today['ë‹¹ì¼_ê°€ê²©%']}%** ìƒìŠ¹í•˜ë©° ì£¼ëª©ë°›ì•˜ìŠµë‹ˆë‹¤.\n"
    else:
        worst_today = df.loc[df['ë‹¹ì¼_ê°€ê²©%'].idxmin()]
        summary += f"### ğŸ“‰ ë‹¹ì¼ ì‹œì¥ ë™í–¥\n"
        summary += f"- ì „ë°˜ì ìœ¼ë¡œ ì•½ì„¸ì¥ì´ì—ˆìœ¼ë©°, **{worst_today['ì„¹í„°']}** ì„¹í„°ì˜ í•˜ë½í­ì´ ì»¸ìŠµë‹ˆë‹¤.\n"
    summary += "\n"

    # 3. ì„¹í„°ë³„ ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½
    summary += "### ğŸ“° ì„¹í„°ë³„ ì£¼ìš” ë‰´ìŠ¤\n"
    for sector in ["ë°˜ë„ì²´", "ì´ì°¨ì „ì§€", "ìë™ì°¨", "ê¸ˆìœµ"]: # ì£¼ìš” ì„¹í„°ë§Œ ìš”ì•½ ë…¸ì¶œ
        if sector in sector_news:
            summary += f"**[{sector}]**\n"
            summary += "\n".join(sector_news[sector][:2]) + "\n"
    
    summary += "\n---"
    return summary

def main():
    print("=" * 80)
    print(f"í•œêµ­ ì¦ì‹œ ì„¹í„°ë³„ ì¢…í•© ë¦¬í¬íŠ¸ ({datetime.now().strftime('%Y-%m-%d %H:%M')})")
    print("=" * 80)
    
    sectors = get_sector_data()
    results = []
    sector_news_dict = {}
    
    for sector, tickers in sectors.items():
        print(f"ë¶„ì„ ì¤‘: {sector}...")
        metrics = get_stats_yf_and_naver(tickers)
        if not metrics:
            continue
        
        # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        print(f"  - {sector} ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        sector_news_dict[sector] = get_sector_news(sector)
        
        res = {
            "ì„¹í„°": sector,
            "ë‹¹ì¼_ê°€ê²©%": metrics["ë‹¹ì¼"]["ê°€ê²©%"], "ë‹¹ì¼_ê±°ë˜ëŸ‰": metrics["ë‹¹ì¼"]["ê±°ë˜ëŸ‰"],
            "ë‹¹ì¼_ì™¸ì¸": metrics["ë‹¹ì¼"]["ì™¸ì¸"], "ë‹¹ì¼_ê¸°ê´€": metrics["ë‹¹ì¼"]["ê¸°ê´€"], "ë‹¹ì¼_ê°œì¸": metrics["ë‹¹ì¼"]["ê°œì¸"],
            
            "ì–´ì œ_ê°€ê²©%": metrics["ì–´ì œ"]["ê°€ê²©%"], "ì–´ì œ_ê±°ë˜ëŸ‰": metrics["ì–´ì œ"]["ê±°ë˜ëŸ‰"],
            "ì–´ì œ_ì™¸ì¸": metrics["ì–´ì œ"]["ì™¸ì¸"], "ì–´ì œ_ê¸°ê´€": metrics["ì–´ì œ"]["ê¸°ê´€"], "ì–´ì œ_ê°œì¸": metrics["ì–´ì œ"]["ê°œì¸"],
            
            "ì£¼ê°„_ê°€ê²©%": metrics["ì£¼ê°„"]["ê°€ê²©%"], "ì£¼ê°„_ê±°ë˜ëŸ‰": metrics["ì£¼ê°„"]["ê±°ë˜ëŸ‰"],
            "ì£¼ê°„_ì™¸ì¸": metrics["ì£¼ê°„"]["ì™¸ì¸"], "ì£¼ê°„_ê¸°ê´€": metrics["ì£¼ê°„"]["ê¸°ê´€"], "ì£¼ê°„_ê°œì¸": metrics["ì£¼ê°„"]["ê°œì¸"]
        }
        results.append(res)
        
    if results:
        df = pd.DataFrame(results).fillna(0)
    else:
        print("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # Generate Summary with News
    analysis_summary = generate_summary(df, sector_news_dict)
    
    # Markdown Report
    today_str = datetime.now().strftime('%Y-%m-%d')
    filename = f"reports/report_{today_str}.md"
    os.makedirs("reports", exist_ok=True)
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# í•œêµ­ ì¦ì‹œ ì„¹í„°ë³„ ì¢…í•© ë¦¬í¬íŠ¸ ({today_str})\n\n")
        
        f.write(analysis_summary + "\n\n")
        
        f.write("## ğŸ“Š ì„¹í„°ë³„ ì„¸ë¶€ ì§€í‘œ\n")
        f.write("- ê°€ê²©% : ì„¹í„° ë‚´ ì¢…ëª©ë“¤ì˜ í‰ê·  ê°€ê²© ë³€ë™ë¥ \n")
        f.write("- ê±°ë˜ëŸ‰ : í•´ë‹¹ ê¸°ê°„ ì„¹í„° ë‚´ ì¢…ëª©ë“¤ì˜ ì´ ê±°ë˜ëŸ‰ (ì£¼)\n")
        f.write("- ì™¸ì¸/ê¸°ê´€/ê°œì¸ : í•´ë‹¹ ê¸°ê°„ ì„¹í„° ë‚´ ì¢…ëª©ë“¤ì˜ ìˆœë§¤ìˆ˜ ìˆ˜ëŸ‰ í•©ê³„ (ì£¼)\n\n")
        
        for period in ["ë‹¹ì¼", "ì–´ì œ", "ì£¼ê°„"]:
            f.write(f"### {period} ë¦¬í¬íŠ¸\n\n")
            cols = ["ì„¹í„°"] + [c for c in df.columns if c.startswith(period)]
            sub_df = df[cols].copy()
            sub_df.columns = [c.replace(f"{period}_", "") for c in sub_df.columns]
            for c in sub_df.columns:
                if sub_df[c].dtype == 'int64' or sub_df[c].dtype == 'float64':
                    if c != 'ê°€ê²©%':
                        sub_df[c] = sub_df[c].apply(lambda x: f"{int(x):,}")
            
            f.write(sub_df.to_markdown(index=False))
            f.write("\n\n")
            
        # Add All News at the end
        f.write("## ğŸ” ì„¹í„°ë³„ ì£¼ìš” ë‰´ìŠ¤ ì „ì²´ ë³´ê¸°\n\n")
        for sector, news in sector_news_dict.items():
            f.write(f"### {sector}\n")
            f.write("\n".join(news) + "\n\n")
            
        f.write("*ì´ ë¦¬í¬íŠ¸ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*")

    print(f"\n[ì•Œë¦¼] ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
    print(df.to_string(index=False))

    print(f"\n[ì•Œë¦¼] ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
    print(df.to_string(index=False))

    print(f"\n[ì•Œë¦¼] ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
    print(df.to_string(index=False))

if __name__ == "__main__":
    main()
